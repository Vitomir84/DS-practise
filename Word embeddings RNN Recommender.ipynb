{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZBRUaiBBEpa"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:08.534341Z",
     "iopub.status.busy": "2020-09-23T02:01:08.533598Z",
     "iopub.status.idle": "2020-09-23T02:01:08.535846Z",
     "shell.execute_reply": "2020-09-23T02:01:08.536284Z"
    },
    "id": "YS3NA-i6nAFC"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aojnnc7sXrab"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6mJg1g3apaz"
   },
   "source": [
    "This tutorial contains an introduction to word embeddings. You will train your own word embeddings using a simple Keras model for a sentiment classification task, and then visualize them in the [Embedding Projector](http://projector.tensorflow.org) (shown in the image below). \n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=1\" alt=\"Screenshot of the embedding projector\" width=\"400\"/>\n",
    "\n",
    "## Representing text as numbers\n",
    "\n",
    "Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. In this section, you will look at three strategies for doing so.\n",
    "\n",
    "### One-hot encodings\n",
    "\n",
    "As a first idea, you might \"one-hot\" encode each word in your vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
    "\n",
    "To create a vector that contains the encoding of the sentence, you could then concatenate the one-hot vectors for each word.\n",
    "\n",
    "Key point: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n",
    "\n",
    "### Encode each word with a unique number\n",
    "\n",
    "A second approach you might try is to encode each word using a unique number. Continuing the example above, you could assign 1 to \"cat\", 2 to \"mat\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This appoach is efficient. Instead of a sparse vector, you now have a dense one (where all elements are full).\n",
    "\n",
    "There are two downsides to this approach, however:\n",
    "\n",
    "* The integer-encoding is arbitrary (it does not capture any relationship between words).\n",
    "\n",
    "* An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n",
    "\n",
    "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZUQErGewZxE"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:08.541903Z",
     "iopub.status.busy": "2020-09-23T02:01:08.541265Z",
     "iopub.status.idle": "2020-09-23T02:01:15.053625Z",
     "shell.execute_reply": "2020-09-23T02:01:15.052960Z"
    },
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "### Download the IMDb Dataset\n",
    "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) through the tutorial. You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. To read more about loading a dataset from scratch, see the [Loading text tutorial](../load_data/text.ipynb).  \n",
    "\n",
    "Download the dataset using Keras file utility and take a look at the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:15.060473Z",
     "iopub.status.busy": "2020-09-23T02:01:15.059798Z",
     "iopub.status.idle": "2020-09-23T02:01:35.720462Z",
     "shell.execute_reply": "2020-09-23T02:01:35.721030Z"
    },
    "id": "aPO4_UmfF0KH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 39s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imdbEr.txt', 'test', 'imdb.vocab', 'README', 'train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6yROZNKvbd"
   },
   "source": [
    "Take a look at the `train/` directory. It has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:35.726685Z",
     "iopub.status.busy": "2020-09-23T02:01:35.725501Z",
     "iopub.status.idle": "2020-09-23T02:01:35.730757Z",
     "shell.execute_reply": "2020-09-23T02:01:35.730150Z"
    },
    "id": "9-iOHJGN6SDu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_unsup.txt',\n",
       " 'neg',\n",
       " 'urls_pos.txt',\n",
       " 'unsup',\n",
       " 'urls_neg.txt',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'labeledBow.feat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O59BdioK8jY"
   },
   "source": [
    "The `train` directory also has additional folders which should be removed before creating training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:35.735136Z",
     "iopub.status.busy": "2020-09-23T02:01:35.734512Z",
     "iopub.status.idle": "2020-09-23T02:01:36.651134Z",
     "shell.execute_reply": "2020-09-23T02:01:36.650378Z"
    },
    "id": "1_Vfi9oWMSh-"
   },
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoJjiEyJz9u"
   },
   "source": [
    "Next, create a `tf.data.Dataset` using `tf.keras.preprocessing.text_dataset_from_directory`. You can read more about using this utility in this [text classification tutorial](https://www.tensorflow.org/tutorials/keras/text_classification). \n",
    "\n",
    "Use the `train` directory to create both train and validation datasets with a split of 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:36.658186Z",
     "iopub.status.busy": "2020-09-23T02:01:36.657117Z",
     "iopub.status.idle": "2020-09-23T02:01:40.354927Z",
     "shell.execute_reply": "2020-09-23T02:01:40.354410Z"
    },
    "id": "ItYD3TLkCOP1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, \n",
    "    subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHa6cq0-Ym0g"
   },
   "source": [
    "Take a look at a few movie reviews and their labels `(1: positive, 0: negative)` from the train dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:40.360252Z",
     "iopub.status.busy": "2020-09-23T02:01:40.359563Z",
     "iopub.status.idle": "2020-09-23T02:01:40.703847Z",
     "shell.execute_reply": "2020-09-23T02:01:40.704387Z"
    },
    "id": "aTCbSkvkYmTT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================================\n",
      "label: 0 text: b\"Geordies...salt of the earth characters...bricklayers...beer...Geordies...happy go lucky...adventures working abroad...salt of the earth characters...warm wonderful people...Tyne Bridge (tear in the eye)...brown ale...salt of the earth characters...cute little Red Indians children in Newcastle United tops...emetic...Geordies...salt of the earth characters...<br /><br />etc etc etc....<br /><br />Please. This is so poor. And you should know better Timothy Spall. They can't have paid you that much.<br /><br />As for Jimmy Nail. Well the kindest thing that can be said is that he is every bit as good an 'actor' as he is a singer and writer. Come on Jimmy, the joke's over. 'Crocodile Shoes' and 'Spender' were very funny, unfortunately I don't think they were supposed to be. With 'Auf Wiedersehen Pet' the opposite applies.\"\n",
      "label: 0 text: b\"My girlfriend picked this one; as a southern born and raised African American I found this movie's plot and premise totally without credibility. To believe that class and racial biases would be so easily and comfortably suspended would only come from someone totally unfamiliar with the ante-bellum south. Totally absurd !!! I wonder how they got a good actor like Harvey Keitel and a good actress like Andie McDowell (who being southern knows better) to participate in this crap\"\n",
      "label: 0 text: b\"Is this the movie??? Is this what Indians are trying to show?? I think this is one more effort from a sick-minded director to turn down Pakistani soldiers and in fact country....but what we Pakistani's know that we are always ahead of India in every part of our lives...not only in armed counters.<br /><br />Well...this is bad filmed as that of Border in early 1997...and director and writer just tried to overcome a shame of defeat in Kargil by Pakistani armed forces, by creating films like these..<br /><br />One thing is very clear...Whenever there will be an encounter between Pakistan and India....we will win....!!! So Mr. Dutta try to make some good movies instead of Nonsense movies like this\"\n",
      "label: 0 text: b\"I should say right away that I checked the spoilers box only because I'm giving this comment the amount of thought proportional to what this mess of a movie deserves, and don't want to be held responsible for some plot point incidentally slipping out.<br /><br />This comment will take the form of a tirade for the simple reason that I am still under the influence of this movie, having just watched it, and the unique effect this has renders one incapable of the sort of forethought and paragraph structure required for coherent, reasoned criticism. That is not a compliment. It isn't the narcotic effect of a truly hypnotic or thought provoking movie. The feelings it stirs up combine like some uncomfortable emotional Voltron, composed of a confusing mix of some form of rage, the vague desire to take a shower, the rudderless, sinking feeling of true betrayal one gets when they realize they have given 109 minutes of their lives into the hands of someone who would not only squander it, but do so in such a pompous, artless way. And I probably wouldn't have done anything super productive with that 109 minutes anyway! But even if I'd spent it on something trivial, like a power block of masturbation and online poker, I would have felt more fulfilled when all was said and done.<br /><br />The problems with this movie are myriad, and in better times I'd articulate exactly what they were in a semi-adult fashion. But in keeping with what this movie deserves, I think I'll most likely stick to the realm of masturbation jokes and cartoon references.<br /><br />The most irritating and terminal flaw is that while watching this movie one is keenly aware that the makers and participants think they are making a much smarter movie than they are. Demonstrating the depth of knowledge one could pick up in a one semester survey of Western art history at a community college or trade school, the art-jargon is piled on thick and from all directions, with much of it supplied by talk between our hero, the tortured detective Stan (Willem Dafoe, who I will forgive for this movie due to him being Willem Dafoe) and his accented antique dealer buddy Blair (Peter Stormare, taking a break from playing a sociopath for whom murder comes easy by playing a 2-dimensional plot device in a movie about a sociopath for whom murder comes easy). And talk they do. In fact, we are dropped into this story at a crime scene that may indicate the reemergence of a serial killer Stan thinks he killed years earlier, so all the back story is established partially through unclear flashback, but primarily through stilted conversations between Stan and his dealer, or Stan and his colleague, the unforgivably irritating Carl (Scott Speedman). And although I differentiate the character Carl (Scott Speedman) from the actor who plays him by using parentheses, I must admit that very early on in the film I despised this character so much that I actually found myself sincerely wishing harm on the actor portraying him (Scott Speedman). Not anything too fancy. Not death or paralysis, necessarily.. But maybe herpes? Or maybe a stage light could fall on him and crush his arm? This is a dangerous digression, but I'm not editing it out because I want to leave anyone reading this who's thinking about paying to see this train wreck of a movie with a clear impression of the horrible wishes and feelings it stirs in even the most peaceful man.<br /><br />Well, I'm sort of running out of steam here.. over the course of writing this the sick feelings this movie brought up in a me have subsided, my head has cleared a bit. Realizing now that I'm still investing time in something related to this piece of sh!t is startlingly similar to waking up after a night of suicidally heavy drinking next to the heaving form of a still slumbering 200 pound college girl. Your first urge is a desperate desire to flee. This is natural.\"\n",
      "label: 1 text: b'Someone(or, something thing..)is leaving puncture marks on the jugular and draining victims of their blood till dead. Police detective Karl Brettschneider(Melvyn Douglas, before slipping out of the B-movie horror genre for greater heights)is stumped at who..or what..is behind these notorious crimes. The village is overcome by hysteria and Karl depends on his trusted medical genius, Dr. Otto von Niemann(Lionel Atwill, in yet another effective mad scientist role)to provide some feedback as to what might be causing the deaths of innocents. He also fears for the safety of his beloved Ruth(the lovely Fay Wray who stars for the third time with Atwill after \"Doctor X\" & \"The Mystery of the Wax Museum\")who is Niemann\\'s assistant.<br /><br />Dwight Frye steals the film as a rather loony village idiot who collects bats and carries a demented demeanor wherever he goes..it\\'s easy to see why he becomes a suspect as local paranoia is at a fever pitch. Maude Eburne provides the film\\'s humor as a very naive(..and easily influenced)patient of von Niemann\\'s who believes she has ailments she reads about in books near the laboratory where he works. She\\'s impressionable and often von Niemann just humors her and constant fictional illnesses she feels plagued with. Lionel Belmore returns as yet another frightened, superstitious B\\xc3\\xbcrgermeister.<br /><br />Creaky, static, but rather entertaining nonetheless thanks to the cast. The film is obviously as low-budget as they come, but this doesn\\'t hurt the film too much since it\\'s put together rather well by director Frank R Strayer and his crew. I\\'m certain the film\\'s print has seen better days, though. This is the kind of B-horror item you\\'d find packaged in with 50 other random cheesefests and poverty row programmers. The film\\'s villain..and his motives for feeding a synthetically made biological creature..certainly provides a different take on the Frankenstein formula. Many might be disappointed with the end results as the film strays away from being an actual supernatural tale about a real vampire killer causing the murders.'\n",
      "=====================================================================================================================\n",
      "label: 0 text: b'Generally I don\\'t like films directed by Sydney Pollack (\"The Firm\" being somewhat of an exception) and I\\'ve never been a Robert Redford fan either. Still, I thought \"Three Days of the Condor\" must be good because of the number of praising comments it has received.<br /><br />Although the widescreen cinematography is quite pleasing for the eye and Max von Sydow does a nice job as the sinister professional killer, I found the whole affair tremendously disappointing. The film undeniably radiates paranoid atmosphere, but everything is ruined by the muddled plot which doesn\\'t seem to make any sense.<br /><br />The film also contains one of the most unsatisfying endings I have ever seen which really leaves the viewer hanging in the air. And what can one say about that absurd romance between Redford and Dunaway? I\\'d much rather watch any of Hitchcock\\'s films five times in succession than to sit through this piece of waste once more.'\n",
      "label: 0 text: b\"The script for this TV soap opera is so bad that even A. Hopkins at some point had to play like an undergrad drama-student so as to bring some life in his script-dead character. I do not know whether this was the purpose of the director, but Hopkins' Ciano reeked nothing but vanity, fear and lack of self-esteem. The real Ciano possibly was all that but then, why make a movie about him? Mussolini was a bit more convincing, and his long way down was as if closer to the truth. Edda Mussolini was plain ridiculous (not because of Sarandon, but because of the impotent script), while she had to be the central character of this alleged familial drama. Watch it only if you enjoy Venezuelan soap opera.\"\n",
      "label: 0 text: b\"Sexo Cannibal, or Devil Hunter as it's more commonly known amongst English speaking audiences, starts with actress & model Laura Crawford (Ursula Buchfellner as Ursula Fellner) checking out locations for her new film along with her assistant Jane (Gisela Hahn). After a long days work Laura is relaxing in the bath of her room when two very dubious character's named Chris (Werner Pochath) & Thomas (Antonio Mayans) burst in & kidnap her having been helped by the treacherous Jane. Laura's agent gets on the blower to rent-a-hero Peter Weston (Al Cliver) who is informed of the situation, the kidnappers have Laura on an isolated island & are demanding a 6 million ransom. Peter is told that he will be paid 200,000 to get her back safely & a further 10% of the 6 million if he brings that back as well, faster than a rat up a drain pipe Peter & his Vietnam Vet buddy helicopter pilot Jack are on the island & deciding on how to save Laura. So, the kidnappers have Laura & Peter has the 6 million but neither want to hand them over that much. Just to complicate things further this particular isolated island is home to a primitive tribe (hell, in all the generations they've lived there they've only managed to build one straw hut, now that's primitive) who worship some cannibal monster dude (Burt Altman) with bulging eyes as a God with human sacrifices & this cannibal has a liking for young, white female flesh & intestines...<br /><br />This Spanish, French & German co-production was co-written & directed by the prolific Jesus Franco who also gets the credit for the music as well. Sexo Cannibal has gained a certain amount of notoriety here in the UK as it was placed on the 'Video Nasties' list in the early 80's under it's alternate Devil Hunter title & therefore officially classed as obscene & banned, having said that I have no idea why as it is one bad film & even Franco, who isn't afraid to be associated with a turkey, decides he wants to hide under the pseudonym of Clifford Brown. I'd imagine even the most die-hard Franco fan would have a hard time defending this thing. The script by Franco, erm sorry I mean Clifford Brown & Julian Esteban as Julius Valery who was obviously another one less than impressed with the finished product & wanted his named removed, is awful. It's as simple & straight forward as that. For a start the film is so boring it's untrue, the kidnap plot is one of the dullest I've ever seen without the slightest bit of tension or excitement involved & the horror side of things don't improve as we get a big black guy with stupid looking over-sized bloodshot eyes plus two tame cannibal scenes. As a horror film Sexo Cannibal fails & as an action adventure it has no more success, this is one to avoid.<br /><br />Director Franco shows his usual incompetence throughout, a decapitated head is achieved by an actor lying on the ground with large leaves placed around the bottom of his neck to try & give the impression it's not attached to anything! The cannibal scenes are poor, the action is lame & it has endless scenes of people randomly walking around the jungle getting from 'A' to 'B' & not really doing anything when they get there either. It becomes incredibly dull & tedious to watch after about 10 minutes & don't forget this thing goes on for 94 minutes in it's uncut state. I also must mention the hilarious scene when Al Cliver is supposed to be climbing a cliff, this is achieved by Franco turning his camera on it's side & having Cliver crawl along the floor! Just look at the way his coat hangs & the way he never grabs onto to anything as he just pulls himself along! The gore isn't that great & as far as Euro cannibal films go this is very tame, there are some gross close ups of the cannibals mouth as it chews bits of meat, a man is impaled on spikes, there's some blood & a handful of intestines. There's a fair bit of nudity in Sexo Cannibal & an unpleasant rape scene.<br /><br />Sexo Cannibal must have had a low budget & I mean low. This is a shoddy poorly made film with awful special effects & rock bottom production values. The only decent thing about it is the jungle setting which at least looks authentic. The music sucks & sound effects become annoying as there is lots of heavy breathing whenever the cannibal is on screen. The acting sucks, the whole thing was obviously dubbed anyway but no one in this thing can act.<br /><br />Sexo Cannibal is a terrible film that commits the fatal mistake of being as boring as hell. The only good things I can say is that it has a certain sleazy atmosphere to it & those close ups of the cannibal chewing meat are pretty gross. Anyone looking for a decent cinematic experience should give Sexo Cannibal as wide a berth as possible, one to avoid.\"\n",
      "label: 1 text: b\"I've seen the original English version on video. Disney's choice of voice actors looks very promising. I can't believe I'm saying that. The story is about a young boy who meets a girl with a history that is intertwined with his own. The two are thrown into one of the most fun and intriguing storylines in any animated film. The animation quality is excellent! If you've seen Disney's job of Kiki's delivery service you can see the quality in their production. It almost redeems them for stealing the story of Kimba the white lion. (but not quite!) Finally Miyazaki's films are being released properly! I can't wait to see an uncut English version of Nausicaa!\"\n",
      "label: 0 text: b'I read in the papers that W.Snipes was broke so no wonder he would take parts in low budget projects like The Contractor.He is just the next action star to join a growing club:the penniless action stars of the 90s (Van Damme,Segal,Lundgren,Snipes). Here he stars the lead in a cheap action flick which was shot in Bulgaria( we are supposed to believe that the location is London, like only a complete moron would buy that)The story is the one of 1000 other movies: retired special forces good guy gets hired by the government again to do a wet job- after that government wants to get rid of him- good guy gets away after killing bad guys (was that a spoiler? guess not!) The star of the movie: the little girl (Eliza Bennett) outperforms everybody else of the cast!!!One star is for her plus one star for eye candy Lena Headey, makes 2 stars. Only for die hard Snipes fans!Everybody else:avoid!'\n",
      "=====================================================================================================================\n",
      "label: 0 text: b\"Maybe television will be as brutal one day. Maybe \\xc2\\x84Big Brother` was only the first step in the direction Stephen \\xc2\\x84Richard Bachmann` King described the end point of. But enough about that. If I spend too much words talking about the serious background topic of this movie I do exactly what the producers hoped by choosing this material. It's the same with \\xc2\\x84The 6th Day`. No matter, how primitive the film is, it provokes a discussion about its topic, which serves the producers as publicity. Let's NOT be taken in by that. The social criticism that is suggested by that plot summary is only an alibi to make it possible to produce a speculative, violent movie, more for video sale than for cinema. <br /><br />I didn't read the book. I don't dare criticising Stephen King without having read him, but when I saw the film I thought they couldn't make such a terrible film out of a good book: In a typical 1980s set with 1980s music and some minor actors Arnold Schwarzenegger finds himself as a policeman running away from killers within a cruel TV show. The audience is cheering.<br /><br />Together with \\xc2\\x84Predator`, this is definitely Schwarzenegger's most stupid movie. 2 stars out of 10.\"\n",
      "label: 0 text: b'After seeing the previews I felt that this movie was going to be a nice improvement over that fast & furious series. So, I already expected it to have a lacking storyline, but at least this time it won\\'t be loaded with a bunch of powerless civics with fart cans. Unfortunately, I was wrong. If you could only imagine a Fast & Furious movie with a worse story line than by all means this movie is for you.<br /><br />This is the absolute worst movie I had ever seen (I\\'m being nice - no I would not take baseball bat to my nuts like what others have said). Not only was the storyline non-existent, but the action was crap too. I guess the director thought that they could just throw bunch of females and exotic cars and then call it a movie. For an example, there is a point in the movie where the guy pushes the nos button and his Lamborghini takes off in the air and flies over a SLR McLaren to win. And after the bit where Eddy Griffin got in a fight with one of his \"girls\" (an Imus comment would work in this case) the girl asks to pull over and get out of the PLANE and of course they do in the middle of the desert. After this wonderful scene I couldn\\'t take it anymore. So, I only got to see half of this monstrosity. This is the first movie I had ever walked out on. Afterwards, I had to stop for some drinks to kill all of my corrupted brain cells.<br /><br />I gave it a 1 because 0 is not an option. You\\'re better off going to the local car show and stopping at a strip joint on the way home. I will keep all viewers in my prayers.'\n",
      "label: 1 text: b\"This is a good film for 99% of the duration. I feel that the ending has occluded this film from higher acclaim.<br /><br />It is shot in a rather naive fashion. This is clearly done to create a more chilling feel to the film - a feeling of isolation becomes apparent very soon on due to this filming technique.<br /><br />The gruesome characters are very well acted and presented especially the 'nutcase' called Joe. However, the wholesome (normal) characters are a little too pathetic for my liking - granted, they are supposed to come across as pathetic but this is done a little OTT.<br /><br />The film starts slowly (and the naive camera work smacks of 'B' movie to start with) and very normally but you soon get a feel of the impending brutality that is about to occur. This is one of the most 'twisted' movies with respect to cold-hearted violence.<br /><br />After the abrupt and unbelievably lazy ending I was left feeling disappointed. I would have given the film a 9 if the ending was in keeping with the rest of the film but as it is it gets only a 7 on the strength of the 'eeriness' and nail-biting scenes earlier on in the film.<br /><br />Give it a watch and excuse the ending!\"\n",
      "label: 0 text: b'The concept of this made-for-TV horror movie is ludicrous beyond words, but hey, it was the late 1970\\'s and literally all stupid horror formats were pretty damn profitable, so why not exploit the idea of a satanically possessed dog? The plot of \"Devil Dog\" is easy to describe to fans of the horror genre: simply think of \"The Omen\" and replace the newborn baby boy with a nest of German Shepard pups! Seriously, I\\'m not kidding, that\\'s what the movie is about! During the opening sequence, members of some kind of satanic cult buy a female dog in heat only to have it impregnated by Satan himself. You\\'d think that the Lord of Darkness has other things on His mind than to fornicate with a German Shepard and take over the world one evil puppy at the time, but apparently not. Exactly like little Damien in \"The Omen\", one of the puppies is taken in by model family and grows up to become a beautiful and charismatic animal. But Lucky \\xc2\\x96 that\\'s the dog\\'s name \\xc2\\x96 is pure evil and liquidates annoying neighbors and nosy school teachers in derivative and tamely executed ways. He also inflicts his malignant character on the family wife and children, but he cannot force the father (Richard Crenna) to stick his arm into a lawnmower because he\\'s a \"chosen one\". The whole thing becomes too moronic for words when Crenna eventually travels to Ecuador to search for an ancient wall painting and gets advice from an old witchdoctor who speaks perfect English. I guess he learned that living in isolation atop of a mountain his entire life. Director Curtis Harrington (\"What\\'s the matter with Helen\", \"Ruby\") and lead actor Richard Crenna (\"Wait until Dark\", \"The Evil\") desperately try to create a suspenseful and mysterious atmosphere, but all is in vain. Scenes like cute puppy eyes spontaneously setting fire to a Spanish maid or a dog dodging bullets without even moving evoke chuckles instead of frights, and not even spooky musical tunes can chance that. The \"special\" effects are pathetic, especially near the end when the Satan-dog mutates into an utterly cheesy shadow on the wall. \"Devil Dog\" is a truly dumb movie, but it\\'s definitely hilarious to watch late at night with some friends and loads of liquor. There are entertaining brief cameos of Martine Beswick (\"Dr. Jekyll and Sister Hyde\") as the terrifying cult queen and R.G. Armstrong (\"The Car\", \"The Pack\") as the evil fruit, vegetable and puppy salesman. And, yes, that annoying daughter is the same kid who gets blown away complaining about her ice-cream in Carpenter\\'s \"Assault on Precinct 13\".'\n",
      "label: 1 text: b'In New York, the family man dentist Alan Johnson (Don Cheadle) meets his former roommate and friend Charlie Fineman (Adam Sandler) by chance on the street. Charlie became a lonely and deranged man after the loss of his wife and three daughters in the tragic September 11th while Alan has problems to discuss his innermost feelings with his wife. Alan reties his friendship with Charlie and they become close to each other. Alan tries to fix Charlie\\'s life, sending him to the psychologist Angela Oakhurst (Liv Tyler), but Charlie has an aggressive reaction to the treatment and is send to court.<br /><br />\"Reign Over Me\" is a good drama about loss, friendship, family and loneliness. The September 11th is irrelevant to the plot; it could be a car accident, a fire or any other tragedy, as well as the sexual harassment of Donna Remar, played by the gorgeous Saffron Burrows, to Alan. But the family drama works, supported by the great performances of Adam Sandler and Don Cheadle. Liv Tyler is quite impossible to be recognized, I do not know whether she is using excessive make-up to look older, but her face is weird. My vote is seven.<br /><br />Title (Brazil): \"Reine Sobre Mim\" (\"Reign Over Me\")'\n",
      "=====================================================================================================================\n",
      "label: 0 text: b'\"Proximity\" tells of a convict (Lowe) who thinks the prison staff is out to kill him. This very ordinary film is an action/drama with a weak plot; stereotypical, poorly developed characters; and a one dimensional performance by Lowe. A forgettable film not worthy of further commentary.'\n",
      "label: 1 text: b\"All Dogs Go To Heaven Is The Most Cutest Animated Film To Have Dogs In 1989. The Previous Don Bluth Film The Land Before Time(1988) Became A Success. Dogs Are So Cute As Little Mice. Aw, I Just Want To Hug Them When They're Cute. Where Was I? Oh, Yes. Its Animation Is Beautiful, The Characters Are Great When They're Perfectly Voiced And The Songs Are Cute And Touching. It Opened In November 17 1989 The Same Date As The Little Mermaid Produced By Walt Disney Feature Animation.<br /><br />The Part Where Charlie Got Killed By Carface Was So Unforgivable. Carface Is So Mean Because He Wanted To Kill Charlie. Shame On Him! The Love Survive Song Performed By Irene Cara And Freddie Jackson Was So Beautiful. All Dogs Go To Heaven Is The Best Animated Movie Ever.\"\n",
      "label: 0 text: b\"Some wonder why there weren't anymore Mrs. Murphy movies after this one. Will it's because this movie totally blew snot. Disney was not the right studio to run this film. MAYBE Touchstone (well, they're owned by Disney, but it'd be more adult). The film is too kid-ish, as the book series is not. The casting is all wrong for the characters. The characters don't even act the way they do in the books. And why was Tucker changed to a guy? He's a girl in the frigging books! Was this done to make the film appeal to boys? Sheesh. And where was Pewter, the gray cat? One of the funniest characters from the book is absent from this filth. Rita Mae Brown is a good writer, but letting Disney blow her work was wrong. An animated feature film, perhaps in the vane of Don Bluth's artwork would suit a better Mrs. Murphy film. Overall, I give this a 2, because at least Disney made a film from an under-appreciated book series. But, I wish they did better. Either way, I still have my books to entertain me.\"\n",
      "label: 0 text: b'SUcks. That\\'s all I got to say about this sorry excuse for a film. Sucks. Sucks. Sucks. I mean, what the hell were they thinking? The idiots involved should never be allowed to make another films. The acting was so bad that it even failed to entertain on a bad level. The attempt at a \"lesbian scene\" was sad. I felt so bad for the ladies involved. This movie sucks! Sucks! Sucks!<br /><br />I heard rumors of a sequel.<br /><br />God<br /><br />Help<br /><br />Us<br /><br />All'\n",
      "label: 0 text: b\"Bobby is a goofy kid who smiles far too much and wants sex. So he buys a van to aid in this quest. The acting is lame, the comedy is pathetic and the script is no more than a loosely strung chain of clich\\xc3\\xa9s and cheap thrills. The makers of the film obviously wanted to capture some of the out there craziness of other films of the time, but fell a long way short. They even resort to Bobby slipping on a banana skin, because this will supposedly add comedic value.<br /><br />I'm struggling to find a redeeming feature of the film. If you like DeVito, this is another classic DeVito kind of role - but he's only a supporting actor and there for clich\\xc3\\xa9 value.\"\n",
      "=====================================================================================================================\n",
      "label: 1 text: b\"Rich, alcoholic Robert Stack falls in love with secretary Lauren Bacall. He marries her and is so happy he stops drinking. However, Bacall is secretly loved by Stacks' best friend, Rock Hudson. And Stacks' nymphomaniac sister, Dorothy Malone, lusts after Rock. Throw in a few complications and the movie goes spinning out of control (in a good way).<br /><br />Very glossy movie in beautiful Technicolor with jaw-dropping fashions and furnishings (check out Bacall's hotel room at the beginning). Everybody looks perfect and dresses in beautiful, form-fitting clothes. Basically this is a soap opera with grade A production values. The story itself is lots of fun and some of the dialogue at the beginning is hilariously over the top. The acting by Hudson, Stack and Bacall isn't that good, but seeing them so young and glamorous is great...especially Stack...when he smiled my knees went weak! Dorothy Malone, on the other hand, is fantastic--she deservedly won Best Supporting Actress for her role. She's sexy, violent, vicious and sympathetic...all convincingly. <br /><br />Fun, glossy trash. Don't miss it!\"\n",
      "label: 0 text: b'The movie starts little cute. There are a number of revolting scenes. People in toilets. GOOD actors wasted and the original television series has all but ruined here. This did not need to be crude.<br /><br />Forget it. Find the tv show. Disney at new low.'\n",
      "label: 0 text: b\"Jack London's life was certainly colorful enough for a dozen films about different aspects of him. Sad to say though that what his life was used for in film was some wartime propaganda that put the best face on some of the least attractive parts of his character.<br /><br />Jack London who barely saw the age of 40 when he died wrote some of the best stories around. He wrote on what he knew, but he also wrote as does everyone else bringing the baggage of his own life experience with him. Some of that experience in another day and time would have been condemned as racism. But this was World War II and London was a big believer in the 'yellow peril' as it was called back in the day.<br /><br />Two thirds of the film covers his life as author, we see his years as a seaman from where he got the inspiration for The Sea Wolf. We see him up in the Yukon in a miner's cabin with a dog that was no doubt his inspiration for The Call of the Wild. London was able to capture the spirit of adventure that his own life was all about right on paper for the world to enjoy ever since.<br /><br />The final third dealt with his time as a war correspondent covering the Russo-Japanese War. London was a socialist, but his socialism did not encompass folks who were Oriental. Like a few million others he saw the rising immigration of the Chinese and Japanese to our Pacific coast as a threat to jobs for the white people. He advocated strict immigration policies for Orientals.<br /><br />The film puts the cart before the horse. London is presented as a man who saw because he was on hand at the Russo-Japanese War what Japan's ambitions were and for that reason was as xenophobic as he was. Actually the kind of atrocities present in World War II were not existent during the Russo-Japanese conflict. Japan had her imperial ambitions, but so did everyone else including the USA at that time. But our immigration policies caused by pressure from our West Coast politicians was a big contributing factor to the deterioration of relations with Japan over a couple of generations. London was part of the cause not a prophet crying in the wilderness.<br /><br />This film was the first independent production of Samuel Bronston who later did some films with a bit more budget than Jack London. Had he a bit more money Bronston might have gotten James Cagney or Spencer Tracy, both who would have been right for the role. Instead they got Michael O'Shea who was making his second film after Lady of Burlesque. O'Shea is fine in the part, but certainly was no box office.<br /><br />As London is covering the war, he meets up with a Captain Tanaka who is played by Leonard Strong, an actor who specialized in Orientals and played a ton of them in World War II. From the vantage point in 1905 Strong outlines in the best Fu Manchu tradition Japan's imperial aims right up to taking on the USA eventually. Must have gone over great with the swing shift crowd. <br /><br />A lot of course is left out of London's life including a first wife. Playing the second and only wife in this film is Susan Hayward who only comes into the movie when it's half over. I wish we'd have seen more of her. Charmian Kittredge London survived her husband by almost 40 years dying in 1955.<br /><br />O'Shea in fact met and married the leading lady of his life in Jack London. Virginia Mayo has a small role in Jack London and they married for 30 years until O'Shea died in 1973.<br /><br />Maybe one day we'll get a view of Jack London that will be a lot better than this one.\"\n",
      "label: 0 text: b\"This is the best film the Derek couple has ever made and if you think this is a recommendation then you haven't seen any of the others. There are the usual ingredients: it is just as poorly acted as their other efforts, we can watch Bo disrobing or auditioning for wet T-shirt contests quite frequently, the story is just laughably idiotic, and the film takes itself much too seriously. And then: Orang Utans in Africa?<br /><br />But it has a few things going for it. Bo looks great, the production values (sets, costumes, etc.) are quite good, and this greatly enhances its camp value. In a strange way it is actually quite funny, simply because it tries to be serious and fails so badly.\"\n",
      "label: 0 text: b'I wondered why John Wood was not playing Dr. Falken until I watched the film. BAD plot, bad science, bad acting and overall a bad film. Please don\\'t watch this film. Rent the original \"War Games\" if you are feeling nostalgic.<br /><br />I didn\\'t like the bending of the plot to beat-the-terrorist-threat idea either. In the first film W.O.P.R was built because Russia had 1000s of warheads pointed at the U.S.A. In this film the idea behind the computer was to kill terrorist in training before they are a threat. Politics aside, one of the good thing about the first film was the highlighting that even a stupid computer could grasp the idea of the pointlessness of war in the end. No such insight is offered in this film.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in val_ds.take(5):\n",
    "    print(39*'===')\n",
    "    for i in range(5):\n",
    "        print('label:', label_batch[i].numpy(), 'text:', text_batch.numpy()[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHV2pchDhzDn"
   },
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
    "\n",
    "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
    "\n",
    "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:40.709299Z",
     "iopub.status.busy": "2020-09-23T02:01:40.708670Z",
     "iopub.status.idle": "2020-09-23T02:01:40.712420Z",
     "shell.execute_reply": "2020-09-23T02:01:40.711911Z"
    },
    "id": "Oz6k1IW7h1TO"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "## Using the Embedding layer\n",
    "\n",
    "Keras makes it easy to use word embeddings. Take a look at the [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
    "\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:40.721292Z",
     "iopub.status.busy": "2020-09-23T02:01:40.716010Z",
     "iopub.status.idle": "2020-09-23T02:01:40.725307Z",
     "shell.execute_reply": "2020-09-23T02:01:40.724825Z"
    },
    "id": "-OjxLVrMvWUE"
   },
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dKKV1L2Rk7e"
   },
   "source": [
    "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n",
    "\n",
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:40.730057Z",
     "iopub.status.busy": "2020-09-23T02:01:40.729380Z",
     "iopub.status.idle": "2020-09-23T02:01:40.738500Z",
     "shell.execute_reply": "2020-09-23T02:01:40.738915Z"
    },
    "id": "0YUjPgP7w0PO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03969515,  0.00913128, -0.01237369, -0.014854  , -0.03349719],\n",
       "       [-0.03257857, -0.04720243, -0.04500348,  0.01121489, -0.04858403],\n",
       "       [ 0.00255441,  0.04587946, -0.01244842,  0.03174751,  0.03226903]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1,2,3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4PC4QzsxTGx"
   },
   "source": [
    "For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15).\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:40.743308Z",
     "iopub.status.busy": "2020-09-23T02:01:40.742629Z",
     "iopub.status.idle": "2020-09-23T02:01:40.746183Z",
     "shell.execute_reply": "2020-09-23T02:01:40.745652Z"
    },
    "id": "vwSYepRjyRGy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses pooling because it's the simplest. The [Text Classification with an RNN](text_classification_rnn.ipynb) tutorial is a good next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGicgV5qT0wh"
   },
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6NZSqIIoU0Y"
   },
   "source": [
    "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. You can learn more about using this layer in the [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:40.761922Z",
     "iopub.status.busy": "2020-09-23T02:01:40.761212Z",
     "iopub.status.idle": "2020-09-23T02:01:44.212102Z",
     "shell.execute_reply": "2020-09-23T02:01:44.211368Z"
    },
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "## Create a classification model\n",
    "\n",
    "Use the [Keras Sequential API](../../guide/keras) to define the sentiment classification model. In this case it is a \"Continuous bag of words\" style model.\n",
    "* The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) layer transforms strings into vocabulary indices. You have already initialized `vectorize_layer` as a TextVectorization layer and built it's vocabulary by calling `adapt` on `text_ds`. Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding tranformed strings into the Embedding layer.\n",
    "* The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n",
    "\n",
    "* The [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
    "\n",
    "* The fixed-length output vector is piped through a fully-connected ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer with 16 hidden units.\n",
    "\n",
    "* The last layer is densely connected with a single output node. \n",
    "\n",
    "Caution: This model doesn't use masking, so the zero-padding is used as part of the input and hence the padding length may affect the output.  To fix this, see the [masking and padding guide](../../guide/keras/masking_and_padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:44.225109Z",
     "iopub.status.busy": "2020-09-23T02:01:44.224356Z",
     "iopub.status.idle": "2020-09-23T02:01:44.231286Z",
     "shell.execute_reply": "2020-09-23T02:01:44.230699Z"
    },
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLNgKO7W2fe"
   },
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpX9etB6IOQd"
   },
   "source": [
    "You will use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize metrics including loss and accuracy. Create a `tf.keras.callbacks.TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:44.236595Z",
     "iopub.status.busy": "2020-09-23T02:01:44.235972Z",
     "iopub.status.idle": "2020-09-23T02:01:44.238668Z",
     "shell.execute_reply": "2020-09-23T02:01:44.238113Z"
    },
    "id": "W4Hg3IHFt4Px"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrKAKAKIbuH"
   },
   "source": [
    "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:01:44.250853Z",
     "iopub.status.busy": "2020-09-23T02:01:44.250076Z",
     "iopub.status.idle": "2020-09-23T02:01:44.259188Z",
     "shell.execute_reply": "2020-09-23T02:01:44.258675Z"
    },
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0209 - accuracy: 0.9984 - val_loss: 1.0316 - val_accuracy: 0.7900\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0205 - accuracy: 0.9984 - val_loss: 1.0376 - val_accuracy: 0.7900\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0201 - accuracy: 0.9985 - val_loss: 1.0434 - val_accuracy: 0.7892\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0197 - accuracy: 0.9985 - val_loss: 1.0491 - val_accuracy: 0.7888\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 0.0194 - accuracy: 0.9987 - val_loss: 1.0547 - val_accuracy: 0.7886\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0191 - accuracy: 0.9987 - val_loss: 1.0602 - val_accuracy: 0.7868\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0189 - accuracy: 0.9987 - val_loss: 1.0657 - val_accuracy: 0.7858\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0187 - accuracy: 0.9987 - val_loss: 1.0714 - val_accuracy: 0.7852\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0185 - accuracy: 0.9987 - val_loss: 1.0773 - val_accuracy: 0.7844\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0184 - accuracy: 0.9986 - val_loss: 1.0833 - val_accuracy: 0.7834\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0183 - accuracy: 0.9986 - val_loss: 1.0896 - val_accuracy: 0.7828\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0182 - accuracy: 0.9987 - val_loss: 1.0962 - val_accuracy: 0.7820\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0183 - accuracy: 0.9987 - val_loss: 1.1033 - val_accuracy: 0.7808\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0182 - accuracy: 0.9987 - val_loss: 1.1108 - val_accuracy: 0.7808\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0182 - accuracy: 0.9987 - val_loss: 1.1190 - val_accuracy: 0.7808\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0182 - accuracy: 0.9987 - val_loss: 1.1280 - val_accuracy: 0.7826\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0181 - accuracy: 0.9987 - val_loss: 1.1389 - val_accuracy: 0.7832\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 0.0180 - accuracy: 0.9987 - val_loss: 1.1526 - val_accuracy: 0.7846\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0178 - accuracy: 0.9987 - val_loss: 1.1710 - val_accuracy: 0.7870\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0176 - accuracy: 0.9986 - val_loss: 1.1958 - val_accuracy: 0.7898\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0174 - accuracy: 0.9984 - val_loss: 1.2265 - val_accuracy: 0.7922\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0174 - accuracy: 0.9985 - val_loss: 1.2580 - val_accuracy: 0.7924\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0176 - accuracy: 0.9985 - val_loss: 1.2827 - val_accuracy: 0.7918\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0178 - accuracy: 0.9981 - val_loss: 1.2925 - val_accuracy: 0.7922\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0177 - accuracy: 0.9984 - val_loss: 1.2837 - val_accuracy: 0.7908\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0171 - accuracy: 0.9987 - val_loss: 1.2609 - val_accuracy: 0.7908\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0159 - accuracy: 0.9988 - val_loss: 1.2367 - val_accuracy: 0.7870\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0140 - accuracy: 0.9990 - val_loss: 1.2252 - val_accuracy: 0.7850\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0125 - accuracy: 0.9992 - val_loss: 1.2282 - val_accuracy: 0.7828\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0117 - accuracy: 0.9991 - val_loss: 1.2392 - val_accuracy: 0.7838\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0109 - accuracy: 0.9993 - val_loss: 1.2532 - val_accuracy: 0.7850\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0102 - accuracy: 0.9994 - val_loss: 1.2646 - val_accuracy: 0.7862\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0097 - accuracy: 0.9994 - val_loss: 1.2719 - val_accuracy: 0.7860\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0094 - accuracy: 0.9994 - val_loss: 1.2777 - val_accuracy: 0.7856\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0092 - accuracy: 0.9994 - val_loss: 1.2829 - val_accuracy: 0.7852\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 0.0090 - accuracy: 0.9995 - val_loss: 1.2882 - val_accuracy: 0.7848\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0089 - accuracy: 0.9995 - val_loss: 1.2938 - val_accuracy: 0.7844\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0087 - accuracy: 0.9995 - val_loss: 1.2995 - val_accuracy: 0.7844\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0086 - accuracy: 0.9995 - val_loss: 1.3052 - val_accuracy: 0.7842\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 0.0085 - accuracy: 0.9995 - val_loss: 1.3108 - val_accuracy: 0.7844\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0083 - accuracy: 0.9995 - val_loss: 1.3164 - val_accuracy: 0.7846\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0082 - accuracy: 0.9995 - val_loss: 1.3219 - val_accuracy: 0.7856\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0081 - accuracy: 0.9995 - val_loss: 1.3274 - val_accuracy: 0.7856\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0080 - accuracy: 0.9995 - val_loss: 1.3329 - val_accuracy: 0.7852\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0078 - accuracy: 0.9995 - val_loss: 1.3387 - val_accuracy: 0.7854\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 0.0076 - accuracy: 0.9995 - val_loss: 1.3446 - val_accuracy: 0.7852\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 1.3502 - val_accuracy: 0.7852\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0073 - accuracy: 0.9995 - val_loss: 1.3555 - val_accuracy: 0.7850\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0072 - accuracy: 0.9995 - val_loss: 1.3608 - val_accuracy: 0.7844\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0070 - accuracy: 0.9995 - val_loss: 1.3664 - val_accuracy: 0.7848\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0068 - accuracy: 0.9995 - val_loss: 1.3729 - val_accuracy: 0.7850\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0066 - accuracy: 0.9995 - val_loss: 1.3795 - val_accuracy: 0.7846\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 1.3853 - val_accuracy: 0.7846\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0062 - accuracy: 0.9996 - val_loss: 1.3914 - val_accuracy: 0.7844\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0059 - accuracy: 0.9996 - val_loss: 1.3976 - val_accuracy: 0.7836\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 0.0058 - accuracy: 0.9997 - val_loss: 1.4033 - val_accuracy: 0.7836\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0057 - accuracy: 0.9997 - val_loss: 1.4087 - val_accuracy: 0.7836\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0055 - accuracy: 0.9997 - val_loss: 1.4141 - val_accuracy: 0.7838\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 0.0055 - accuracy: 0.9997 - val_loss: 1.4196 - val_accuracy: 0.7840\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0054 - accuracy: 0.9997 - val_loss: 1.4252 - val_accuracy: 0.7842\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0053 - accuracy: 0.9997 - val_loss: 1.4307 - val_accuracy: 0.7836\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 1.4362 - val_accuracy: 0.7840\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0051 - accuracy: 0.9997 - val_loss: 1.4416 - val_accuracy: 0.7842\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 0.0050 - accuracy: 0.9997 - val_loss: 1.4471 - val_accuracy: 0.7842\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0049 - accuracy: 0.9997 - val_loss: 1.4528 - val_accuracy: 0.7840\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 0.0048 - accuracy: 0.9997 - val_loss: 1.4587 - val_accuracy: 0.7842\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 0.0047 - accuracy: 0.9997 - val_loss: 1.4638 - val_accuracy: 0.7840\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 1.4679 - val_accuracy: 0.7842\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 1.4734 - val_accuracy: 0.7842\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 1.4793 - val_accuracy: 0.7844\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 1.4855 - val_accuracy: 0.7844\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 1.4914 - val_accuracy: 0.7842\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 1.4967 - val_accuracy: 0.7842\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 1.5021 - val_accuracy: 0.7846\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 1.5076 - val_accuracy: 0.7842\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 1.5127 - val_accuracy: 0.7842\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 1.5176 - val_accuracy: 0.7842\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 1.5236 - val_accuracy: 0.7838\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0034 - accuracy: 0.9999 - val_loss: 1.5299 - val_accuracy: 0.7838\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 1.5355 - val_accuracy: 0.7836\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 1.5409 - val_accuracy: 0.7840\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 1.5463 - val_accuracy: 0.7838\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 1.5518 - val_accuracy: 0.7838\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 1.5573 - val_accuracy: 0.7840\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 1.5627 - val_accuracy: 0.7842\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 1.5681 - val_accuracy: 0.7840\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 1.5735 - val_accuracy: 0.7840\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 1.5792 - val_accuracy: 0.7834\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0028 - accuracy: 0.9999 - val_loss: 1.5859 - val_accuracy: 0.7836\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 1.5921 - val_accuracy: 0.7834\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0026 - accuracy: 0.9999 - val_loss: 1.5974 - val_accuracy: 0.7836\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 1.6022 - val_accuracy: 0.7834\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 1.6072 - val_accuracy: 0.7832\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 1.6126 - val_accuracy: 0.7830\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 1.6181 - val_accuracy: 0.7830\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 1.6235 - val_accuracy: 0.7830\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 1.6289 - val_accuracy: 0.7828\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 1.6342 - val_accuracy: 0.7828\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 1.6395 - val_accuracy: 0.7828\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 1.6448 - val_accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wYnVedSPfmX"
   },
   "source": [
    "With this approach the model reaches a validation accuracy of around 84% (note that the model is overfitting since training accuracy is higher).\n",
    "\n",
    "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n",
    "\n",
    "You can look into the model summary to learn more about each layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:02:08.647540Z",
     "iopub.status.busy": "2020-09-23T02:02:08.646898Z",
     "iopub.status.idle": "2020-09-23T02:02:08.650238Z",
     "shell.execute_reply": "2020-09-23T02:02:08.649607Z"
    },
    "id": "mDCgjWyq_0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 46ms/step - loss: 1.6448 - accuracy: 0.7826\n",
      "Loss:  1.6448242664337158\n",
      "Accuracy:  0.7825999855995178\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the model for making new predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiQbOJZ2WBFY"
   },
   "source": [
    "Visualize the model metrics in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNyLjcoRWEue"
   },
   "source": [
    "```\n",
    "%tensorboard --logdir logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 36334), started 0:23:53 ago. (Use '!kill 36334' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-116943cb07c41078\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-116943cb07c41078\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvURkGVpXDOy"
   },
   "source": [
    "![embeddings_classifier_accuracy.png](images/embeddings_classifier_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCoA6qwqP836"
   },
   "source": [
    "## Retrieve the trained word embeddings and save them to disk\n",
    "\n",
    "Next, retrieve the word embeddings learned during training. The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape `(vocab_size, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:02:08.655221Z",
     "iopub.status.busy": "2020-09-23T02:02:08.654583Z",
     "iopub.status.idle": "2020-09-23T02:02:08.674921Z",
     "shell.execute_reply": "2020-09-23T02:02:08.674392Z"
    },
    "id": "n-j0db7ipF6k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(vocab[:10])\n",
    "# Get weights matrix of layer named 'embedding'\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "print(weights.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MiCA77X8B8"
   },
   "source": [
    "Write the weights to disk. To use the [Embedding Projector](http://projector.tensorflow.org), you will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T02:02:08.681782Z",
     "iopub.status.busy": "2020-09-23T02:02:08.681124Z",
     "iopub.status.idle": "2020-09-23T02:02:08.820048Z",
     "shell.execute_reply": "2020-09-23T02:02:08.820525Z"
    },
    "id": "GsjempweP9Lq"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(vocab):\n",
    "    if num == 0: \n",
    "        continue # skip padding token from vocab\n",
    "    vec = weights[num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('vecs.tsv')\n",
    "    files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQyMZWyxYjMr"
   },
   "source": [
    "If you are running this tutorial in [Colaboratory](https://colab.research.google.com), you can use the following snippet to download these files to your local machine (or use the file browser, *View -> Table of contents -> File browser*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXLfFA54Yz-o"
   },
   "source": [
    "## Visualize the embeddings\n",
    "\n",
    "To visualize the embeddings, upload them to the embedding projector.\n",
    "\n",
    "Open the [Embedding Projector](http://projector.tensorflow.org/) (this can also run in a local TensorBoard instance).\n",
    "\n",
    "* Click on \"Load data\".\n",
    "\n",
    "* Upload the two files you created above: `vecs.tsv` and `meta.tsv`.\n",
    "\n",
    "The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\". \n",
    "\n",
    "Note: Experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the `Dense(16)` layer, retraining the model, and visualizing the embeddings again.\n",
    "\n",
    "Note: Typically, a much larger dataset is needed to train more interpretable word embeddings. This tutorial uses a small IMDb dataset for the purpose of demonstration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvKiEHjramNh"
   },
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSgAZpwF5xF_"
   },
   "source": [
    "This tutorial has shown you how to train and visualize word embeddings from scratch on a small dataset.\n",
    "\n",
    "* To learn more about word embeddings and their mathematical representations, refer to this Stanford [note](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).\n",
    "\n",
    "* To learn more about advanced text processing, read the [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
