{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs,  1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "if gpus:\n",
    "    \n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs, \", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_concat.csv')\n",
    "train_df = train_df.iloc[:len(train_df)//2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(data, train):\n",
    "    image_name = tf.strings.join(['data/train/train/', data['image_name'], '.jpg'])\n",
    "    label = data['target']\n",
    "    \n",
    "    if train:\n",
    "        image = tf.io.read_file(image_name)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image, label = image_augmentations(image, label)\n",
    "#    image = tf.image.resize(image, [224, 224])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfrecord_size(tfrecord):\n",
    "    return sum(1 for _ in tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentations(image,label):\n",
    "    img_size = (224, 224)\n",
    "    \n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if tf.random.uniform([], 0, 1.0, dtype = tf.float32) > 0.75:\n",
    "        image = tf.image.transpose(image)\n",
    "    \n",
    "    probablity_rotation = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    if probablity_rotation > 0.75:\n",
    "        image = tf.image.rot90(image, k = 3)\n",
    "    elif probablity_rotation > 0.5:\n",
    "        image = tf.image.rot90(image, k = 2)\n",
    "    elif probablity_rotation > 0.25:\n",
    "        image = tf.image.rot90(image, k = 1)\n",
    "        \n",
    "    if tf.random.uniform([], 0, 1.0, dtype = tf.float32) >= 0.4:\n",
    "        image = tf.image.random_saturation(image, lower = 0.8, upper = 1.2)\n",
    "    if tf.random.uniform([], 0, 1.0, dtype = tf.float32) >= 0.4:\n",
    "        image = tf.image.random_contrast(image, lower = 0.8, upper = 1.2)\n",
    "    if tf.random.uniform([], 0, 1.0, dtype = tf.float32) >= 0.4:\n",
    "        image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "    \n",
    "    probability_cropping = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    if probability_cropping > 0.7:\n",
    "        if probability_cropping > 0.9:\n",
    "            image = tf.image.central_crop(image, central_fraction = 0.7)\n",
    "        elif probability_cropping > 0.8:\n",
    "            image = tf.image.central_crop(image, central_fraction = 0.8)\n",
    "        else:\n",
    "            image = tf.image.central_crop(image, central_fraction = 0.9)\n",
    "    elif probability_cropping > 0.5:\n",
    "        crop_size = tf.random.uniform([], int(img_size[0] * 0.8), img_size[0], dtype = tf.int32)\n",
    "        image = tf.image.random_crop(image, size = [crop_size, crop_size, 3])\n",
    "\n",
    "    image = tf.image.resize(image, size = img_size)\n",
    "#    image = tf.reshape(image, [*img_size, 3])\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df, train, batchsize):\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices(dict(df[['image_name', 'target']]))\n",
    "    return tf_ds.map(partial(parse_image, train=True), num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(1000).batch(batchsize).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFFICIENTNET B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnetb0():\n",
    "    \n",
    "    K.reset_uids()\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=(224, 224, 3))\n",
    "    \n",
    "    effnet = EfficientNetB0(include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    effnet = effnet(inputs)\n",
    "    pooling = keras.layers.GlobalAveragePooling2D()(effnet)\n",
    "    x = keras.layers.Dense(1000, activation='relu')(pooling)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    dropout = keras.layers.Dropout(0.4)(x)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred, alpha=1.0, gamma=2.0):\n",
    "    \n",
    "    alpha = tf.convert_to_tensor(alpha, dtype=K.floatx())\n",
    "    gamma = tf.convert_to_tensor(gamma, dtype=K.floatx())\n",
    "    \n",
    "#     y_true = tf.convert_to_tensor(y_true)\n",
    "#     y_pred = tf.convert_to_tensor(y_pred)\n",
    "    \n",
    "    loss = keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    pt = K.exp(-loss)\n",
    "    \n",
    "    loss = alpha*(1-pt)**gamma*loss\n",
    "    return K.mean(loss)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfn(epoch, bs=24, epochs=10):\n",
    "    \n",
    "    LR_START = 1e-6\n",
    "    LR_MAX = 2e-4\n",
    "    LR_FINAL = 1e-6\n",
    "    LR_RAMPUP_EPOCHS = 4\n",
    "    LR_SUSTAIN_EPOCHS = 0\n",
    "    DECAY_EPOCHS = epochs  - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n",
    "    LR_EXP_DECAY = (LR_FINAL / LR_MAX) ** (1 / (epochs - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1))\n",
    "\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = LR_START + (LR_MAX + LR_START) * (epoch / LR_RAMPUP_EPOCHS) ** 2.5\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        epoch_diff = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n",
    "        decay_factor = (epoch_diff / DECAY_EPOCHS) * math.pi\n",
    "        decay_factor= (tf.math.cos(decay_factor).numpy() + 1) / 2        \n",
    "        lr = LR_FINAL + (LR_MAX - LR_FINAL) * decay_factor\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=999, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = skf.split(X=np.zeros(len(train_df)), y=train_df.loc[: ,['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(folds,1):\n",
    "    \n",
    "    train_data = get_dataset(train_df.iloc[train_idx].reset_index(drop=True), True, 24)\n",
    "    validation_data = get_dataset(train_df.iloc[val_idx].reset_index(drop=True), True, 24)\n",
    "    train_size = get_tfrecord_size(train_data)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        \n",
    "        model = efficientnetb0()\n",
    "        \n",
    "        opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "#        loss = keras.losses.binary_crossentropy\n",
    "\n",
    "        model.compile(loss=focal_loss, optimizer=opt, metrics=['acc', tf.keras.metrics.AUC()])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    cb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"backup.h5\",monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "    cb_earlystop = keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', \n",
    "                                                 patience=4, restore_best_weights=True, verbose=1)\n",
    "\n",
    "#    cb_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    cb_lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=1)\n",
    "    \n",
    "    params = {\"epochs\":10,\n",
    "              \"validation_data\": validation_data,\n",
    "              \"callbacks\": [cb_earlystop, cb_checkpoint, cb_lr]} \n",
    "    \n",
    "    \n",
    "    history = model.fit(train_data, **params)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
